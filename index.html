<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gesture2Speech: Multimodal Mixture of Experts for Gesture-Driven Speech Synthesis</title>
    <meta name="description" content="We've presented Clarity, a minimalist and elegant website template for AI research.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="https://shikun.io/projects/clarity" property="og:url">
    <meta content="Clarity" property="og:title">
    <meta content="Website Template for AI Research" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:description" content="Clarity: A Minimalist Website Template for AI Research">
    <meta name="twitter:image:src" content="assets/figures/clarity.png">
    
    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script>  <!-- Comment to remove table of content. -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
    <!-- Title Page -->
    <!-- Dark Theme Example: Change the background colour dark and change the following div "blog-title" into "blog-title white". -->
    <div class="container blog" id="first-content" style="background-color: #E0E4E6;">
        <!-- If you don't have a project cover: Change "blog-title" into "blog-title no-cover"  -->
        <div class="blog-title no-cover">
            <div class="blog-intro">
                <div>
                    <h1 class="title">Gesture2Speech: Multimodal Mixture of Experts for Gesture-Driven Speech Synthesis</h1>
                    <p class="author">
                        Anonymous Submission
                    </p>
                   
                    <p class="abstract", style="text-align: justify">
                        We present a novel multi-modal text-to-speech (TTS) framework that leverages visual gesture cues to modulate prosody in a synthesized speech. Motivated by the observation that confident, expressive speakers often coordinate gestures with vocal prosody, we introduce a multi-modal mixture-of-experts architecture that dynamically fuses linguistic content and hand gesture features within a dedicated style extraction module. This fused representation conditions an LLM-based speech decoder, enabling prosodic control that temporally aligns closely with hand gestural movements. To achieve fine-grained synchrony between gestures and prosodic contours, we propose an alignment loss that explicitly models the temporal correspondence between the two modalities. Our approach, evaluated on the PATS dataset, outperforms state-of-the-art baselines in both speech naturalness and gesture-speech synchrony. To the best of the authors' knowledge, this is the first work to leverage hand gesture signals for prosody control in neural speech synthesis.
                    </p>
                    <!-- Using FontAwesome Pro -->
                    <!-- <div class="info">
                        <div>
                            <a href="https://arxiv.org" class="button icon" style="background-color: rgba(255, 255, 255, 0.3)"> Paper <i class="far fa-book-open"></i></a> &nbsp;&nbsp; 
                            <a href="https://github.com" class="button icon" style="background-color: rgba(255, 255, 255, 0.3)">Code <i class="far fa-code"></i></a>  &nbsp;&nbsp; 
                            <a href="https://www.microsoft.com/en-gb/microsoft-365/powerpoint" class="button icon" style="background-color: rgba(255, 255, 255, 0.3);">Slides <i class="far fa-presentation"></i></a>  &nbsp;&nbsp; 
                            <a href="https://huggingface.co/spaces" class="button icon" style="background-color: rgba(255, 255, 255, 0.3)">Demo <i class="fa-light fa-face-smiling-hands"></i></a>
                        </div>
                    </div> -->

                    <!-- Using FontAwesome Free -->
                    <div class="info">
                        <div>
                            <a  class="button icon" style="background-color: rgba(255, 255, 255, 0.2)"> Paper <i class="fa-solid fa-book-open"></i></a> &nbsp;&nbsp; 
                            <a  class="button icon" style="background-color: rgba(255, 255, 255, 0.2)">Code <i class="fa-solid fa-laptop-code"></i></a> 
                        </div>
                    </div>
                </div>
               
                <div class="info">
                    <p>Submitted at Neurips 2025</p>
                </div>
            </div>

        </div>
    </div>

 <style>
    .demo-section {
        width: 100%;
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
        box-sizing: border-box;
    }

    table {
        width: 100%;
        border-collapse: collapse;
        text-align: center;
        table-layout: fixed;
        word-wrap: break-word;
    }

    th, td {
        border: 1px solid #ddd;
        padding: 10px;
        word-break: break-word;
    }

</style>

    
    <div class="container blog main first full-width" id="blog-main">

        <h1 >
            Demo Samples
        </h1>

        <style>
        body {
            font-family: Arial, sans-serif;
            margin: 30px;
        }
        .input-text {
            font-size: 1.2em;
            margin-bottom: 20px;
            padding: 10px;
            background-color: #f9f9f9;
            border-left: 5px solid #4CAF50;
        }
        .section-description {
            font-style: italic;
            margin-bottom: 30px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            text-align: center;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px;
        }
        video, audio {
            width: 100%;
            max-width: 350px;
        }
        th {
            background-color: #f2f2f2;
        }
        .model-name {
            font-weight: bold;
            background-color: #fafafa;
        }
    </style>
</head>
<body>
<h4> Input Text 1:  “In a spark of reaction, atoms collide, transforming matter with invisible power” </h4>

<table>
  <tr>
    <th>Descirption</th>
    <th>Gesture 1</th>
    <th>Gesture 2</th>
    <th>Gesture 3</th>
  </tr>

  <!-- Video Row 1 -->
  <tr>
    <td class="model-name">Reference Video</td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip1_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip2_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip3_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
  </tr>

  <!-- Video Row 2 -->
  <tr>
    <td class="model-name">Extracted Gesture</td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture1_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture2_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture3_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
  </tr>

  <!-- Audio Row 3 -->
  <tr>
    <td class="model-name">Gesture2Speech: XTTS-V2</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_1.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_2.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_3.wav"></audio></td>
  </tr>

  <!-- Audio Row 4 -->
  <tr>
    <td class="model-name">Gesture2Speech: GPT-SoVITS</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_1.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_2.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_3.wav"></audio></td>
  </tr>

  <!-- Audio Row 5 -->
  <tr>
    <td class="model-name">Gesture2Speech: Unimodal MoE</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_1.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_2.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_3.wav"></audio></td>
  </tr>

  <!-- Audio Row 6 -->
  <tr>
    <td class="model-name">Gesture2Speech: H-MoE</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_1.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_2.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_3.wav"></audio></td>
  </tr>

  <!-- Audio Row 7 -->
  <tr>
    <td class="model-name">Gesture2Speech: Multimodal-MoE: Ours</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_1.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_2.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_3.wav"></audio></td>
  </tr>
</table>



<h4>
  Input Text 2:  “Everything changes, nothing stays the same forever.” </h4>
<table>
  <tr>
    <th>Descirption</th>
    <th>Gesture 1</th>
    <th>Gesture 2</th>
    <th>Gesture 3</th>
  </tr>

  <!-- Video Row 1 -->
  <tr>
    <td class="model-name">Reference Video</td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip4_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip5_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip6_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
  </tr>

  <!-- Video Row 2 -->
  <tr>
    <td class="model-name">Extracted Gesture</td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture4_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture5_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture6_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
  </tr>

  <!-- Audio Row 3 -->
  <tr>
    <td class="model-name">Gesture2Speech: XTTS-V2</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_4.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_5.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_6.wav"></audio></td>
  </tr>

  <!-- Audio Row 4 -->
  <tr>
    <td class="model-name">Gesture2Speech: GPT-SoVITS</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_4.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_5.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_6.wav"></audio></td>
  </tr>

  <!-- Audio Row 5 -->
  <tr>
    <td class="model-name">Gesture2Speech: Unimodal MoE</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_4.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_5.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_6.wav"></audio></td>
  </tr>

  <!-- Audio Row 6 -->
  <tr>
    <td class="model-name">Gesture2Speech: H-MoE</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_4.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_5.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_6.wav"></audio></td>
  </tr>

  <!-- Audio Row 7 -->
  <tr>
    <td class="model-name">Gesture2Speech: Multimodal-MoE: Ours</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_4.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_5.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_6.wav"></audio></td>
  </tr>
</table>



<h4>
  Input Text 3: “Technology is evolving so quickly that it's changing the way we live, work, and communicate every day.” </h4>
<table>
  <tr>
    <th>Descirption</th>
    <th>Gesture 1</th>
    <th>Gesture 2</th>
    <th>Gesture 3</th>
  </tr>

  <!-- Video Row 1 -->
  <tr>
    <td class="model-name">Reference Video</td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip7_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip8_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Clip9_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
  </tr>

  <!-- Video Row 2 -->
  <tr>
    <td class="model-name">Extracted Gesture</td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture7_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture8_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
    <td>
      <video controls>
        <source src="Gesture-Neurips25-Demo\Gesture9_GT_fixed.mp4" type="video/mp4">
      </video>
    </td>
  </tr>

  <!-- Audio Row 3 -->
  <tr>
    <td class="model-name">Gesture2Speech: XTTS-V2</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_7.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_8.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model1_9.wav"></audio></td>
  </tr>

  <!-- Audio Row 4 -->
  <tr>
    <td class="model-name">Gesture2Speech: GPT-SoVITS</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_7.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_8.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model2_9.wav"></audio></td>
  </tr>

  <!-- Audio Row 5 -->
  <tr>
    <td class="model-name">Gesture2Speech: Unimodal MoE</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_7.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_8.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model3_9.wav"></audio></td>
  </tr>

  <!-- Audio Row 6 -->
  <tr>
    <td class="model-name">Gesture2Speech: H-MoE</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_7.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_8.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model4_9.wav"></audio></td>
  </tr>

  <!-- Audio Row 7 -->
  <tr>
    <td class="model-name">Gesture2Speech: Multimodal-MoE: Ours</td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_7.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_8.wav"></audio></td>
    <td><audio controls src="Gesture-Neurips25-Demo\Model5_9.wav"></audio></td>
  </tr>
</table>



  </div>


    <!-- Footer Page -->
    <footer>
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>.
            </p>
        </div>    
    </footer>
    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>    
    <script src="assets/scripts/main.js"></script>    
    </html>
</body>
